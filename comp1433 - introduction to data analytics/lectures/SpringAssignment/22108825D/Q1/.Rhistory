ggplot(negative_bin_df, aes(x)) +
geom_point(aes(y = negative_bin), color = "red") +
geom_point(aes(y = theory_sim), color = "blue") +
ggtitle("Simulated vs Theoretical Negative Binomial Distribution") +
xlab("Number of Failures until 20th Success") +
ylab("Probability") +
scale_color_manual(labels = c("Simulated", "Theoretical"),
values = c("red", "blue"))
# Set seed
set.seed(42)
install.packages("ggplot2")
library(ggplot2)
# Set seed
set.seed(42)
# Theoretical probabilities=
theory_sim <- c(NA)
for (i in 0:100) {
theory_sim[i] <- choose((i + 20 - 1), (20 - 1)) * (0.5^20) * ((1 - 0.5)^i)
}
# Simulate the negative binomial distribution
x <- 1:100
n <- 10000
r <- 20
p <- 0.5
negative_sim <- rnbinom(n, size = r, prob = p)
negative_bin <- rep(0, length(x))
for (i in 1:length(x)) {
negative_bin[i] <- sum(negative_sim == x[i]) / length(negative_sim)
}
negative_bin_df <- data.frame(x = 1:100,
negative_bin = negative_bin,
theory_sim = theory_sim)
# Scatter plot of simulated negative binomial distribution
library(ggplot2)
ggplot(negative_bin_df, aes(x)) +
geom_point(aes(y = negative_bin), color = "red") +
geom_point(aes(y = theory_sim), color = "blue") +
ggtitle("Simulated vs Theoretical Negative Binomial Distribution") +
xlab("Number of Failures until 20th Success") +
ylab("Probability") +
scale_color_manual(labels = c("Simulated", "Theoretical"),
values = c("red", "blue"))
# (a) Load the data
employees <- read.csv("employees.csv")
install.packages("ggplot2")
library(ggplot2)
# (a) Load the data
employees <- read.csv("/employees.csv")
install.packages("ggplot2")
install.packages("ggplot2")
library(ggplot2)
# (a) Load the data
employees <- read.csv("/employees.csv")
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("ggplot2")
# Load the iris dataset
irisData <- read.csv("/Users/jay/Documents/Jay/university/year-1/sem-2/comp1433/SpringAssignment/Assignment Data/iris.csv", header = FALSE)
# Load the iris dataset
irisData <- read.csv("/Users/jay/Documents/Jay/university/year-1/sem-2/comp1433/SpringAssignment/Assignment Data/iris.csv", header = FALSE)
# Euclidean distance
E_Distance <- function (X1, Y1, X2, Y2){
dist<-sqrt((X2 - X1)^2+(Y2 - Y1)^2)
return(dist)
}
irisRow <- nrow(iris)
k <-3
distanceList<-matrix(nrow=max_iterations, ncol=k)
# Load the iris dataset
irisData <- read.csv("/Users/jay/Documents/Jay/university/year-1/sem-2/comp1433/SpringAssignment/Assignment Data/iris.csv", header = FALSE)
# Euclidean distance
E_Distance <- function (X1, Y1, X2, Y2){
dist<-sqrt((X2 - X1)^2+(Y2 - Y1)^2)
return(dist)
}
irisRow <- nrow(iris)
k <-3
distanceList<-matrix(nrow=max_iterations, ncol=k)
# K-means clustering algorithm
for(a in 1: 100){
irisData <- data.frame()
for (j in 1: irisRow){
PetalWidth <- iris$Petal.Width[j]
PetalLength <- iris$Petal.Length[j]
tempData <- c(1,E_Distance(PetalLength, PetalWidth, centroids[1,1], centroids[1,2]))
for (i in 2: k){
if (E_Distance(PetalLength, PetalWidth, centroids[i,1], centroids[i,2])<tempData[2]){
tempData <- c(i, E_Distance(PetalLength, PetalWidth, centroids[i,1], centroids[i,2]))
}
}
irisData<- rbind(irisData, c(tempData[1], tempData[2], PetalLength, PetalWidth))
}
for (i in 1: k){
irisSubset<- subset(irisData, irisData[,1] ==i)
meanDistance <- mean( irisSubset[,2])
distanceList[a,i] <- meanDistance
centroids[i, ] <- c(mean( irisSubset[,3]), mean( irisSubset[,4]))
}
}
